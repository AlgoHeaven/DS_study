{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나이브 베이즈: '필터'의 원리.\n",
    "    * 스팸 메시지 거르는 데 많이 쓰이는 방식."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13-1. 바보 스팸 필터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가정\n",
    "    - 가능한 모든 메시지에서 임의로 메시지를 선택하는 '공간'이 있다고 해 보자.\n",
    "    - S = 메시지가 스팸인 경우\n",
    "    - V = 메시지에 '비아그라'라는 단어가 포함되어 있는 경우\n",
    "\n",
    "* 베이즈 정리에 따라, 메시지에 '비아그라'라는 단어가 포함되었을 때 해당 메시지가 스팸일 확률을 표현하면?\n",
    "        - P(S | V) = [ P(V | S) * P(S) ] / [ { P(V | S) * P(S) } + { P(V | !S) * P(!S) } ]\n",
    "     ==> 메시지가 스팸이면서 비아그라 단어를 포함하고 있을 확률 / 메시지에 비아그라라는 단어가 포함되어있을 확률\n",
    "     \n",
    "* 수많은 스팸 메시지와 스팸이 아닌 메시지를 갖고 있고, 메시지가 스팸일 확률(P(S))과 스팸이 아닐 확률(P(!S))이 0.5로 동일하다면, 위 식은 다음과 같다.\n",
    "        - P(S | V) = [ P(V | S) ] / [ P(V | S)  + P(V | !S) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13-2. 조금 더 똑똑한 스팸 필터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가정\n",
    "    * 이제 더 많은 단어 w1, w2, ...., wN이 주어졌다고 해 보자.\n",
    "    * Xi = 단어 wi가 메시지에 포함된 경우\n",
    "    * 스팸 메시지에 i번째 단어가 포함되어 있는 확률 = P(Xi | S)\n",
    "    * 스팸이 아닌 메시지에 i번째 단어가 포함되어있는 확률 = P(Xi | !S)\n",
    "    \n",
    "* 나이브 베이즈의 핵심: '메시지가 스팸이냐 아니냐가 주어졌다는 조건 하에 각 단어의 존재/부재 여부는 서로 조건부 독립적'이다.\n",
    "        = 어떤 스팸 메시지에 A라는 단어를 포함하고 있다는 점이, 그 메시지에 B라는 단어가 포함되어 있는지를 판단하는 데 도움을 주지 않는다.\n",
    "        = P(X1 = x1, ... , Xn = xn | S) = P(X1 = x1 | S) * ... * P(Xn = xn | S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* '나이브 베이즈'에서는 극단적인 가정을 한다.\n",
    "    - 사전에 수록된 단어가 '비아그라', '롤렉스' 뿐이라고 한다.\n",
    "    - 모든 스팸 메시지의 절반은 '값싼 비아그라', 아니면 '정품 롤렉스'에 대한 메시지라고 하자.\n",
    "            - 이 경우, 스팸 메시지에 '비아그라'와 '롤렉스'라는 단어가 포함된 확률을 다음과 같이 추정할 수 있다.\n",
    "               = P(X1 = 1, X2 = 1 | S) = P(X1 = 1 | S) * P(X2 = 1 | S) = 0.5 * 0.5 = 0.25\n",
    "    - 현실에서는 '비아그라'와 '롤렉스'가 동시에 등장하지 않는다는 가정이 없었기 때문에 이러한 확률이 나오는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x라는 단어만으로 스팸 메시지를 걸러내는 필터에서, 메시지가 스팸일 확률을 베이즈 정리를 통해 구하면:\n",
    "            - P(S | X = x) = P(X = x | S) / [ P(X = x | S) + P(X = x | !S)]\n",
    "            - (스팸 메시지 중 x라는 단어가 포함될 확률) / [ (스팸 메시지 중 x라는 단어가 포함될 확률) + (스팸이 아닌 메시지 중 x라는 단어가 포함될 확률) ]\n",
    "            \n",
    "* 간단하게 각 단어가 메시지에 포함될 확률을 모두 곱해서, 위식의 우변의 확률값을 구할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단, 끊임없이 확률을 곱하는 것을 실제로 구현할 시 컴퓨터가 0에 가까운 부동소수점을 제대로 처리하지 못하는 언더플로우(underflow) 현상이 발생한다.\n",
    "* 언더플로우 현상을 방지하기 위해, p1 * ... * pN은 다음과 같이 계산한다.\n",
    "            - 수학 공식에 따르면 log(ab) = log(a) + log(b) 이고 exp( log(x) ) = x 이므로,\n",
    "              exp( log(p1) + ... + log(pN) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이제, 스팸이나 스팸이 아닌 메시지에 단어 wi가 포함될 확률인, P( Xi | S )와 P( Xi | !S)의 값을 추정하는 문제가 남았다.\n",
    "* 만약, 이미 스팸 메시지 / 스팸이 아닌 메시지로 분류된 학습 데이터가 충분히 주어졌다면, P( Xi | S )를 추정할 수 있는 가장 간단한 방법은 스팸 메시지 중 wi가 포함된 메시지의 비율을 계산하는 것이다. 그러나, 이는 다음과 같은 문제를 야기할 수 있다.\n",
    "* 주어진 학습 데이터, 'A'라는 단어는 스팸이 아닌 메시지에만 포함되어있다고 해 보자. 즉, P( 'A' | S ) = 0 이다. 이러한 조건 하에서 새롭게 주어진 메시지의 내용에 'A'와 '비아그라'가 함께 나올 경우, 나이브 베이즈 분류기는 이를 스팸메일이 아닌 것으로 예측할 것이다.\n",
    "* 이런 문제를 처리하기 위해, 평활화(smoothing)가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 평활화(smoothing) 사용방법\n",
    "    - 가짜 빈도수(pseudocount) k를 결정하고, 스팸 메시지에서 i번째 단어가 나올 확률을 다음과 같이 추정할 수 있다.\n",
    "            P(Xi | S) = ( k + wi를 포함하고 있는 스팸 수) / ( 2k + 스팸 수)\n",
    "            \n",
    "    - 한편, P(Xi | !S)를 구하는 방법도 비슷하다. 스팸 메시지에서 i번째 단어가 나올 확률을 계산할 때 해당 단어가 포함된 스팸과 포함되지 않은 스팸이 이미 각각 k번씩 나왔다고 가정을 한다.\n",
    "    - 예를 들어, 'A' 라는 단어는 98개의 스팸 문서에서 단 한번도 나타나지 않았지만, k = 1이라면 위 공식에 따르면\n",
    "            P('A' | S) = ( 1 + 0 ) / ( 2 + 0)\n",
    "      이 된다. 즉, '데이터'라는 단어가 포함된 ㅔㅁ시지가 스팸 메시지일 확률을 0이 아닌 다른 확률 값으로 설정할 수 있게 해 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13-3. 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위에서 살펴본 분류기를 실제로 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#우선, 메시지를 단어 단위로 잘라 주는 함수를 구현한다.\n",
    "#1. 메시지를 모두 소문자로 바꾼다.\n",
    "#2. re.findall()을 사용해서 문자, 숫자, 혹은 따옴표(apostrophe)가 포함된 모든 단어를 추출한다.\n",
    "#3. 마지막으로, set()을 사용해 중복되는 단어를 제거한다.\n",
    "\n",
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()                         # 소문자로 변환\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)  # 단어 추출\n",
    "    return set(all_words)                       # 중복되는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 학습 데이터에서 단어의 빈도수를 세는 함수\n",
    "\n",
    "def count_words(training_set):\n",
    "    \"\"\"학습 데이터는 (메시지 내용, 스팸 여부) 형식으로 구성되어 있다고 한다.\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0]) # counts = [스팸 메시지에서 나온 빈도수(spam_count), \n",
    "                                         # 스팸이 아닌 메시지에서 나온 빈도수(non_spam_count)]\n",
    "                                         # 형태의 list를 값으로 사용하는 dict.\n",
    "    for text, is_spam in training_set:   # 각 단어를 key로 사용함.\n",
    "        for word in tokenize(text):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이제, 평활법(smoothing)으로, 단어의 빈도수로 확률 값을 추정한다.\n",
    "#이 함수는 [단어, 스팸 메시지에서 단어가 나올 확률, 스펨이 아닌 메시지에서 단어가 나올 확률] 형태의 list를 반환한다.\n",
    "\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k= 0.5):\n",
    "    \"\"\"단어의 빈도수를 [ 단어, p(w | 스팸), p(w | !스팸) ] 형태로 리턴함.\"\"\"\n",
    "    return [(w,\n",
    "            (spam + k) / (total_spams + 2 * k),\n",
    "            (non_spam + k) / (total_non_spams + 2 * k))\n",
    "           for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마지막으로, 계산된 단어의 확률과 나이브 베이즈의 기본 가정을 사용해서 메시지가 스팸일 확률을 계산한다.\n",
    "import math\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    \n",
    "    #모든 단어에 대해 반복.\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        \n",
    "        # 만약 메시지에 word가 나타나면 해당 단어가 나올 log 확률을 더해 줌.\n",
    "        if word in message_words:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "                \n",
    "        # 만약 메시지에 word가 나타나지 않는다면 해당 단어가 나오지 않을 log 확률을 더해 줌.\n",
    "        # 나오지 않을 확률은 log(1 - 나올 확률) 로 계산.\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "            \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이즈 클래스로 모든 함수를 이용한다.\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k = 0.5):\n",
    "        self.k = k  # smoothing factor\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        #스팸 메시지와 스팸이 아닌 메시지의 개수를 세어 줌\n",
    "        num_spams = len([is_spam\n",
    "                         for message, is_spam in training_set\n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        \n",
    "        #지금까지 만든 함수에 학습 데이터를 적용\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                            num_spams,\n",
    "                                            num_non_spams,\n",
    "                                            self.k)\n",
    "    \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 모델 검증하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SpamAssassin public corpus를 활용한 스팸필터기 모델 검증.\n",
    "* 3개의 폴더(spam, easy_ham, hard_ham)가 구성되어 있는데, 각 폴더에는 수많은 메일이 각각의 하나의 파일로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 제목 구분하기: 모든 제목은 'Subject:'라고 시작하므로 그 부분만 찾으면 된다.\n",
    "import glob, re\n",
    "\n",
    "#실제 파일을 저장한 경로로 path를 바꾼다.\n",
    "path = \"../naive_bayes/*/*\"\n",
    "data = []\n",
    "\n",
    "#glob.glob는 주어진 경로에 해당하는 모든 파일 이름을 반환함.\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "    \n",
    "    with open(filename, errors='ignore') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                #\"Subject: \" 부분을 제거하고 나머지 부분을 유지\n",
    "                #subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append((subject, is_spam))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, prob): #분류기 생성을 위한 데이터 분류 함수\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 669, (True, True): 86, (True, False): 40, (False, True): 30})\n"
     ]
    }
   ],
   "source": [
    "#분류기 생성\n",
    "#데이터를 학습 데이터와 평가 데이터로 나누면 분류기를 만들 준비는 끝난다.\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(0)  #예시와 동일한 결과를 얻기 위해서 설정\n",
    "train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)\n",
    "\n",
    "#########################################################################################################\n",
    "#모델의 성능 측정.\n",
    "#[제목, 실제 스팸 여부, 예측된 스팸 확률]\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "             for subject, is_spam in test_data]\n",
    "\n",
    "#메시지가 스팸일 확률이 0.5보다 크면 스팸이라고 하자.\n",
    "#그리고, 예측된 스팸 메시지가 실제 스팸인 경우를 세어 보자.\n",
    "counts = Counter((is_spam, spam_probability > 0.5)\n",
    "                 for _, is_spam, spam_probability in classified)\n",
    "\n",
    "print(counts)\n",
    "#TP: 데이터가 실제 스팸이 스팸이라고 예측된 경우(101건)\n",
    "#FP: 데이터가 햄이 스팸으로 예측된 경우(33건)\n",
    "#FN: 데이터가 햄이 햄으로 예측된 경우(704건)\n",
    "#TN: 데이터가 스팸이 햄으로 예측된 경우(38건)\n",
    "#즉, 정밀도(precision) = 101 / (101 + 33) = 75%\n",
    "#    재현율(recall) = 101 / (101 + 38) = 73%\n",
    "#간단한 모델 치고는 나쁘지 않은 성능이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 보기: 가장 잘못 예측된 결과\n",
    "\n",
    "#스팸일 확률을 오름차순으로 정렬\n",
    "classified.sort(key=lambda row: row[2])\n",
    "\n",
    "#스팸이 아닌 메시지 중에서 스팸일 확률이 가장 높은 메시지\n",
    "spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "\n",
    "#스팸 중에서 스팸일 확률이 가장 낮은 메시지\n",
    "hammiest_spams = list(filter(lambda row: row[1], classified))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spambayes', 0.0013333333333333333, 0.04828734538534729), ('users', 0.0013333333333333333, 0.040675547098001905), ('razor', 0.0013333333333333333, 0.03258801141769743), ('zzzzteana', 0.0013333333333333333, 0.03163653663177926), ('sadev', 0.0013333333333333333, 0.027830637488106564), ('apt', 0.0013333333333333333, 0.02450047573739296), ('perl', 0.0013333333333333333, 0.022121788772597527), ('ouch', 0.0013333333333333333, 0.021170313986679352), ('spamassassin', 0.0013333333333333333, 0.01974310180780209), ('bliss', 0.0013333333333333333, 0.01879162702188392)]\n",
      "[('clearance', 0.025333333333333333, 0.0002378686964795433), ('rates', 0.030666666666666665, 0.0002378686964795433), ('sale', 0.030666666666666665, 0.0002378686964795433), ('systemworks', 0.03333333333333333, 0.0002378686964795433), ('adv', 0.044, 0.0002378686964795433)]\n",
      "[('spambayes', 0.0013333333333333333, 0.04828734538534729), ('users', 0.0013333333333333333, 0.040675547098001905), ('razor', 0.0013333333333333333, 0.03258801141769743), ('zzzzteana', 0.0013333333333333333, 0.03163653663177926), ('sadev', 0.0013333333333333333, 0.027830637488106564)]\n"
     ]
    }
   ],
   "source": [
    "#스팸일 확률이 가장 높은 단어\n",
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"베이즈 정리를 통해 P(S | 메시지가 해당 단어를 포함)을 계산\"\"\"\n",
    "    #word_prob는 word_probabilities 함수에서 계산된 결과\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key = p_spam_given_word)\n",
    "spammiest_words = words[-5:]\n",
    "hammiest_words = words[:5]\n",
    "\n",
    "print(words[:10])\n",
    "print(spammiest_words[:10])\n",
    "print(hammiest_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류기의 성능을 높이는 방법\n",
    "    - 더 많은 데이터로 분류기를 학습시키기.\n",
    "    - 메시지의 제목뿐만 아니라 내용도 활용하기(단, 메시지의 헤더를 처리할 방안을 세워야 함).\n",
    "    - 지금 만든 분류기는 단어가 학습 데이터에서 단 한 번이라도 나왔다면, 모두 사용함. 단어의 최소 빈도수를 설정함으로써 기준보다 적게 나온 단어를 무시할 수 있도록 분류기를 수정할 수 있음.\n",
    "    - 메시지를 단어 단위로 잘라 줄 때, 동의어를 고려해야 한다. 이를 위해 간단한 stemmer(어간)를 분류기에 추가하여, 비슷한 의미의 단어를 동일한 그룹으로 묶어 준다. 단, 좋은 stemmer를 만드는 것은 어려우니 이미 구현된 Porter stemmer(한국의 KoNLPy 형태소 분석기와 같은, 영어권에서의 분석기) 등을 자주 사용한다.\n",
    "    - 모델의 변수는 메시지에 포함된 모든 단어였지만, 다른 변수를 사용해도 무관하다. 메시지에 숫자가 포함되어 있다면, 숫자를 contains:number 같은 문자열로 대체하도록 tokenizer 함수를 수정할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
